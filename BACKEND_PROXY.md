# ðŸ”’ Backend Proxy - Seguridad Empresarial

## DescripciÃ³n

El backend proxy es una capa de seguridad que protege tus API keys de OpenAI y ElevenLabs. Las keys se almacenan en el servidor (Vercel), no en el navegador.

## Arquitectura

```
Frontend (Navegador)
    â†“
    â”œâ”€ EnvÃ­a: { message, token }
    â”œâ”€ Recibe: { response }
    â†“
Backend Proxy (Vercel Functions)
    â†“
    â”œâ”€ Valida token JWT
    â”œâ”€ Aplica rate limiting
    â”œâ”€ Llama a OpenAI/ElevenLabs
    â”œâ”€ Logs de auditorÃ­a
    â†“
OpenAI / ElevenLabs APIs
```

## Rutas de API

### POST /api/chat
Obtener respuesta de ARIA a travÃ©s del proxy seguro.

**Request:**
```json
{
  "message": "Â¿CuÃ¡ndo cobro este mes?",
  "conversationHistory": [...]
}
```

**Headers:**
```
Authorization: Bearer {JWT_TOKEN}
Content-Type: application/json
```

**Response:**
```json
{
  "response": "SegÃºn el calendario de nÃ³minas..."
}
```

**Errores:**
- 401: No autorizado
- 429: Demasiadas solicitudes
- 500: Error del servidor

### POST /api/voice
Generar audio a travÃ©s del proxy seguro.

**Request:**
```json
{
  "text": "Hola, soy ARIA"
}
```

**Headers:**
```
Authorization: Bearer {JWT_TOKEN}
Content-Type: application/json
```

**Response:**
- Audio MP3 binario

**Errores:**
- 401: No autorizado
- 429: Demasiadas solicitudes
- 500: Error del servidor

## Seguridad

### 1. AutenticaciÃ³n
- Todas las rutas requieren JWT token
- Token obtenido de Supabase Auth
- Se valida en cada request

### 2. Rate Limiting
- Chat: 30 requests por minuto por usuario
- Voice: 60 requests por minuto por usuario
- Retorna 429 si se excede el lÃ­mite

### 3. ValidaciÃ³n de Input
- MÃ¡ximo 2000 caracteres para chat
- MÃ¡ximo 1000 caracteres para voice
- Valida tipos de datos

### 4. Logging
- Logs de todos los requests
- InformaciÃ³n: usuario, tamaÃ±o de mensaje, tamaÃ±o de respuesta
- En producciÃ³n: enviar a servicio de logs (Sentry, etc.)

### 5. API Keys Protegidas
- Nunca expuestas en frontend
- Almacenadas en variables de entorno de Vercel
- Solo accesibles en backend

## Variables de Entorno (Vercel)

```env
# OpenAI
OPENAI_API_KEY=sk-proj-your-key-here

# ElevenLabs
ELEVENLABS_API_KEY=sk_your-key-here

# Supabase (para validar tokens)
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_ANON_KEY=your-anon-key
```

## ImplementaciÃ³n en Frontend

### Usar ProxyService

```typescript
import { proxyService } from './services/proxyService'

// Obtener respuesta de ARIA
const response = await proxyService.getAriaResponse(
  'Mi pregunta',
  conversationHistory,
  token
)

// Generar audio
const audioBlob = await proxyService.textToSpeech(
  'Texto a convertir',
  token
)

// Reproducir audio
await proxyService.playAudio(audioBlob)
```

## Despliegue en Vercel

### 1. Estructura de Carpetas
```
aria-hr/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ chat.ts
â”‚   â””â”€â”€ voice.ts
â”œâ”€â”€ src/
â”‚   â””â”€â”€ ...
```

### 2. Configurar Variables en Vercel
- Settings â†’ Environment Variables
- AÃ±adir OPENAI_API_KEY, ELEVENLABS_API_KEY, etc.

### 3. Deploy
```bash
git push origin main
# Vercel desplegarÃ¡ automÃ¡ticamente
```

### 4. Verificar
```bash
# Probar endpoint
curl -X POST https://your-domain.vercel.app/api/chat \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"message":"Hola"}'
```

## Monitoreo

### Logs en Vercel
1. Dashboard â†’ Deployments
2. Seleccionar deployment
3. Click en "Logs"

### MÃ©tricas
- Requests por usuario
- Tiempo de respuesta
- Errores (401, 429, 500)
- Uso de APIs externas

## Mejoras Futuras

### Fase 1 (Actual)
- âœ… Rate limiting en memoria
- âœ… ValidaciÃ³n de input
- âœ… Logging bÃ¡sico

### Fase 2 (PrÃ³xima)
- [ ] Rate limiting con Redis
- [ ] CachÃ© de respuestas
- [ ] MÃ©tricas detalladas
- [ ] Alertas de errores

### Fase 3 (Escalado)
- [ ] AutenticaciÃ³n OAuth
- [ ] Webhooks para eventos
- [ ] AnÃ¡lisis de uso
- [ ] FacturaciÃ³n por API

## Troubleshooting

### "Unauthorized" (401)
- Verificar que el token es vÃ¡lido
- Verificar que el token no ha expirado
- Verificar header Authorization

### "Too many requests" (429)
- Esperar 1 minuto
- Reducir frecuencia de requests
- Contactar soporte para aumentar lÃ­mite

### "Internal server error" (500)
- Revisar logs en Vercel
- Verificar que API keys son vÃ¡lidas
- Verificar que las APIs estÃ¡n disponibles

## Costos

### OpenAI
- Chat: ~$0.003 por 1K tokens
- Con proxy: mismo costo, pero mÃ¡s seguro

### ElevenLabs
- TTS: $0.30 por 1M caracteres
- Con proxy: mismo costo, pero mÃ¡s seguro

### Vercel
- Serverless Functions: incluido en plan Pro
- Ejecutable gratis hasta cierto lÃ­mite

## Comparativa: Con/Sin Proxy

| Aspecto | Sin Proxy | Con Proxy |
|---------|-----------|----------|
| API Keys | En frontend | En servidor |
| Seguridad | Baja | Alta |
| Rate Limiting | No | SÃ­ |
| AuditorÃ­a | No | SÃ­ |
| Costo | Igual | Igual |
| Complejidad | Baja | Media |

---

**RecomendaciÃ³n: Usar proxy en producciÃ³n para corporates**

El proxy aÃ±ade seguridad sin aumentar costos significativamente.
